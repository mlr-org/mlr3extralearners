% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/learner_xgboost_surv_xgboost_cox.R
\name{mlr_learners_surv.xgboost.cox}
\alias{mlr_learners_surv.xgboost.cox}
\alias{LearnerSurvXgboostCox}
\title{Extreme Gradient Boosting Cox Survival Learner}
\description{
eXtreme Gradient Boosting regression using a \strong{Cox Proportional Hazards}
objective.
Calls \code{\link[xgboost:xgb.train]{xgboost::xgb.train()}} from package \CRANpkg{xgboost} with \code{objective}
set to \code{survival:cox} and \code{eval_metric} to \code{cox-nloglik}.
}
\note{
To compute on GPUs, you first need to compile \CRANpkg{xgboost} yourself and link
against CUDA.
See \url{https://xgboost.readthedocs.io/en/stable/build.html#building-with-gpu-support}.
}
\section{Prediction types}{

Three types of prediction are returned for this learner:
\enumerate{
\item \code{lp}: a vector of linear predictors (relative risk scores), one per
observation.
\item \code{crank}: same as \code{lp}.
\item \code{distr}: a survival matrix in two dimensions, where observations are
represented in rows and time points in columns.
By default, the Breslow estimator is used via \code{\link[mlr3proba:breslow]{mlr3proba::breslow()}}.
}
}

\section{Saving this learner}{

In order to save a \code{LearnerSurvXgboostCox} for later usage,
it is necessary to call the \verb{$marshal()} method on the \code{Learner}
before writing it to disk, as the object will otherwise not be saved correctly.
After loading a marshaled \code{LearnerSurvXgboostCox} into R again,
you then need to call \verb{$unmarshal()} to transform it into a useable state.
}

\section{Initial parameter values}{

\itemize{
\item \code{nrounds} is initialized to 1000.
\item \code{nthread} is initialized to 1 to avoid conflicts with parallelization via \CRANpkg{future}.
\item \code{verbose} is initialized to 0.
}
}

\section{Dictionary}{

This \link[mlr3:Learner]{Learner} can be instantiated via \link[mlr3:mlr_sugar]{lrn()}:

\if{html}{\out{<div class="sourceCode">}}\preformatted{lrn("surv.xgboost.cox")
}\if{html}{\out{</div>}}
}

\section{Meta Information}{

\itemize{
\item Task type: \dQuote{surv}
\item Predict Types: \dQuote{crank}, \dQuote{distr}, \dQuote{lp}
\item Feature Types: \dQuote{integer}, \dQuote{numeric}
\item Required Packages: \CRANpkg{mlr3}, \CRANpkg{mlr3proba}, \CRANpkg{mlr3extralearners}, \CRANpkg{xgboost}
}
}

\section{Parameters}{
\tabular{lllll}{
   Id \tab Type \tab Default \tab Levels \tab Range \cr
   alpha \tab numeric \tab 0 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   base_score \tab numeric \tab 0.5 \tab  \tab \eqn{(-\infty, \infty)}{(-Inf, Inf)} \cr
   booster \tab character \tab gbtree \tab gbtree, gblinear, dart \tab - \cr
   callbacks \tab untyped \tab list() \tab  \tab - \cr
   colsample_bylevel \tab numeric \tab 1 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   colsample_bynode \tab numeric \tab 1 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   colsample_bytree \tab numeric \tab 1 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   disable_default_eval_metric \tab logical \tab FALSE \tab TRUE, FALSE \tab - \cr
   early_stopping_rounds \tab integer \tab NULL \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   eta \tab numeric \tab 0.3 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   feature_selector \tab character \tab cyclic \tab cyclic, shuffle, random, greedy, thrifty \tab - \cr
   feval \tab untyped \tab NULL \tab  \tab - \cr
   gamma \tab numeric \tab 0 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   grow_policy \tab character \tab depthwise \tab depthwise, lossguide \tab - \cr
   interaction_constraints \tab untyped \tab - \tab  \tab - \cr
   iterationrange \tab untyped \tab - \tab  \tab - \cr
   lambda \tab numeric \tab 1 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   lambda_bias \tab numeric \tab 0 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   max_bin \tab integer \tab 256 \tab  \tab \eqn{[2, \infty)}{[2, Inf)} \cr
   max_delta_step \tab numeric \tab 0 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   max_depth \tab integer \tab 6 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   max_leaves \tab integer \tab 0 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   maximize \tab logical \tab NULL \tab TRUE, FALSE \tab - \cr
   min_child_weight \tab numeric \tab 1 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   missing \tab numeric \tab NA \tab  \tab \eqn{(-\infty, \infty)}{(-Inf, Inf)} \cr
   monotone_constraints \tab integer \tab 0 \tab  \tab \eqn{[-1, 1]}{[-1, 1]} \cr
   normalize_type \tab character \tab tree \tab tree, forest \tab - \cr
   nrounds \tab integer \tab - \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   nthread \tab integer \tab 1 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   num_parallel_tree \tab integer \tab 1 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   one_drop \tab logical \tab FALSE \tab TRUE, FALSE \tab - \cr
   print_every_n \tab integer \tab 1 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   process_type \tab character \tab default \tab default, update \tab - \cr
   rate_drop \tab numeric \tab 0 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   refresh_leaf \tab logical \tab TRUE \tab TRUE, FALSE \tab - \cr
   sampling_method \tab character \tab uniform \tab uniform, gradient_based \tab - \cr
   sample_type \tab character \tab uniform \tab uniform, weighted \tab - \cr
   save_name \tab untyped \tab - \tab  \tab - \cr
   save_period \tab integer \tab - \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   scale_pos_weight \tab numeric \tab 1 \tab  \tab \eqn{(-\infty, \infty)}{(-Inf, Inf)} \cr
   seed_per_iteration \tab logical \tab FALSE \tab TRUE, FALSE \tab - \cr
   skip_drop \tab numeric \tab 0 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   strict_shape \tab logical \tab FALSE \tab TRUE, FALSE \tab - \cr
   subsample \tab numeric \tab 1 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   top_k \tab integer \tab 0 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   tree_method \tab character \tab auto \tab auto, exact, approx, hist, gpu_hist \tab - \cr
   tweedie_variance_power \tab numeric \tab 1.5 \tab  \tab \eqn{[1, 2]}{[1, 2]} \cr
   updater \tab untyped \tab - \tab  \tab - \cr
   verbose \tab integer \tab 1 \tab  \tab \eqn{[0, 2]}{[0, 2]} \cr
   watchlist \tab untyped \tab NULL \tab  \tab - \cr
   xgb_model \tab untyped \tab - \tab  \tab - \cr
   device \tab untyped \tab - \tab  \tab - \cr
}
}

\section{Early stopping}{

Early stopping can be used to find the optimal number of boosting rounds.
The \code{early_stopping_set} parameter controls which set is used to monitor the
performance.
By default, \code{early_stopping_set = "none"} which disables early stopping.
Set \code{early_stopping_set = "test"} to monitor the performance of the model on
the test set while training.
The test set for early stopping can be set with the \code{"test"} row role in the
\link[mlr3:Task]{mlr3::Task}.
Additionally, the range must be set in which the performance must increase
with \code{early_stopping_rounds} and the maximum number of boosting rounds with
\code{nrounds}.
While resampling, the test set is automatically applied from the \link[mlr3:Resampling]{mlr3::Resampling}.
Not that using the test set for early stopping can potentially bias the
performance scores.
}

\examples{
\dontshow{if (learner_is_runnable("surv.xgboost.cox")) withAutoprint(\{ # examplesIf}
# Define the Learner
learner = lrn("surv.xgboost.cox")
print(learner)

# Define a Task
task = tsk("grace")

# Create train and test set
ids = partition(task)

# Train the learner on the training ids
learner$train(task, row_ids = ids$train)

print(learner$model)
print(learner$importance())

# Make predictions for the test rows
predictions = learner$predict(task, row_ids = ids$test)

# Score the predictions
predictions$score()
\dontshow{\}) # examplesIf}
}
\references{
Chen, Tianqi, Guestrin, Carlos (2016).
\dQuote{Xgboost: A scalable tree boosting system.}
In \emph{Proceedings of the 22nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, 785--794.
ACM.
\doi{10.1145/2939672.2939785}.
}
\seealso{
\itemize{
\item \link[mlr3misc:Dictionary]{Dictionary} of \link[mlr3:Learner]{Learners}: \link[mlr3:mlr_learners]{mlr3::mlr_learners}.
\item \code{as.data.table(mlr_learners)} for a table of available \link[mlr3:Learner]{Learners} in the running session (depending on the loaded packages).
\item Chapter in the \href{https://mlr3book.mlr-org.com/}{mlr3book}: \url{https://mlr3book.mlr-org.com/basics.html#learners}
\item \CRANpkg{mlr3learners} for a selection of recommended learners.
\item \CRANpkg{mlr3cluster} for unsupervised clustering learners.
\item \CRANpkg{mlr3pipelines} to combine learners with pre- and postprocessing steps.
\item \CRANpkg{mlr3tuning} for tuning of hyperparameters, \CRANpkg{mlr3tuningspaces} for established default tuning spaces.
}
}
\author{
bblodfon
}
\section{Super classes}{
\code{\link[mlr3:Learner]{mlr3::Learner}} -> \code{\link[mlr3proba:LearnerSurv]{mlr3proba::LearnerSurv}} -> \code{LearnerSurvXgboostCox}
}
\section{Active bindings}{
\if{html}{\out{<div class="r6-active-bindings">}}
\describe{
\item{\code{internal_valid_scores}}{The last observation of the validation scores for all metrics.
Extracted from \code{model$evaluation_log}}

\item{\code{internal_tuned_values}}{Returns the early stopped iterations if \code{early_stopping_rounds} was set during training.}

\item{\code{validate}}{How to construct the internal validation data. This parameter can be either \code{NULL},
a ratio, \code{"test"}, or \code{"predefined"}.}

\item{\code{marshaled}}{(\code{logical(1)})\cr
Whether the learner has been marshaled.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-LearnerSurvXgboostCox-new}{\code{LearnerSurvXgboostCox$new()}}
\item \href{#method-LearnerSurvXgboostCox-importance}{\code{LearnerSurvXgboostCox$importance()}}
\item \href{#method-LearnerSurvXgboostCox-marshal}{\code{LearnerSurvXgboostCox$marshal()}}
\item \href{#method-LearnerSurvXgboostCox-unmarshal}{\code{LearnerSurvXgboostCox$unmarshal()}}
\item \href{#method-LearnerSurvXgboostCox-clone}{\code{LearnerSurvXgboostCox$clone()}}
}
}
\if{html}{\out{
<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="base_learner"><a href='../../mlr3/html/Learner.html#method-Learner-base_learner'><code>mlr3::Learner$base_learner()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="configure"><a href='../../mlr3/html/Learner.html#method-Learner-configure'><code>mlr3::Learner$configure()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="encapsulate"><a href='../../mlr3/html/Learner.html#method-Learner-encapsulate'><code>mlr3::Learner$encapsulate()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="format"><a href='../../mlr3/html/Learner.html#method-Learner-format'><code>mlr3::Learner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="help"><a href='../../mlr3/html/Learner.html#method-Learner-help'><code>mlr3::Learner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="predict"><a href='../../mlr3/html/Learner.html#method-Learner-predict'><code>mlr3::Learner$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="predict_newdata"><a href='../../mlr3/html/Learner.html#method-Learner-predict_newdata'><code>mlr3::Learner$predict_newdata()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="print"><a href='../../mlr3/html/Learner.html#method-Learner-print'><code>mlr3::Learner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="reset"><a href='../../mlr3/html/Learner.html#method-Learner-reset'><code>mlr3::Learner$reset()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="selected_features"><a href='../../mlr3/html/Learner.html#method-Learner-selected_features'><code>mlr3::Learner$selected_features()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="train"><a href='../../mlr3/html/Learner.html#method-Learner-train'><code>mlr3::Learner$train()</code></a></span></li>
</ul>
</details>
}}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LearnerSurvXgboostCox-new"></a>}}
\if{latex}{\out{\hypertarget{method-LearnerSurvXgboostCox-new}{}}}
\subsection{Method \code{new()}}{
Creates a new instance of this \link[R6:R6Class]{R6} class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LearnerSurvXgboostCox$new()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LearnerSurvXgboostCox-importance"></a>}}
\if{latex}{\out{\hypertarget{method-LearnerSurvXgboostCox-importance}{}}}
\subsection{Method \code{importance()}}{
The importance scores are calculated with \code{\link[xgboost:xgb.importance]{xgboost::xgb.importance()}}.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LearnerSurvXgboostCox$importance()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
Named \code{numeric()}.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LearnerSurvXgboostCox-marshal"></a>}}
\if{latex}{\out{\hypertarget{method-LearnerSurvXgboostCox-marshal}{}}}
\subsection{Method \code{marshal()}}{
Marshal the learner's model.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LearnerSurvXgboostCox$marshal(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{(any)\cr
Additional arguments passed to \code{\link[mlr3:marshaling]{mlr3::marshal_model()}}.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LearnerSurvXgboostCox-unmarshal"></a>}}
\if{latex}{\out{\hypertarget{method-LearnerSurvXgboostCox-unmarshal}{}}}
\subsection{Method \code{unmarshal()}}{
Unmarshal the learner's model.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LearnerSurvXgboostCox$unmarshal(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{(any)\cr
Additional arguments passed to \code{\link[mlr3:marshaling]{mlr3::unmarshal_model()}}.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LearnerSurvXgboostCox-clone"></a>}}
\if{latex}{\out{\hypertarget{method-LearnerSurvXgboostCox-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LearnerSurvXgboostCox$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
